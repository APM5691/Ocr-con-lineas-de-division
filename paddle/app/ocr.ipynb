{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "ff38bbbc-c227-475c-9181-97ff56f5d977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerias Cargadas\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import json\n",
    "import re\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "print(\"Librerias Cargadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2b9ccefe-aec5-45a5-911f-1745cb335035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/PP-LCNet_x1_0_textline_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('latin_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/latin_PP-OCRv5_mobile_rec`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ocr = PaddleOCR(\n",
    "#     use_doc_orientation_classify=False, \n",
    "#     use_doc_unwarping=False, \n",
    "#     use_textline_orientation=False) # text detection + text recognition\n",
    "\n",
    "# text image preprocessing + text detection + textline orientation classification + text recognition\n",
    "ocr = PaddleOCR(use_doc_orientation_classify=False, use_doc_unwarping=False, lang=\"es\")\n",
    "\n",
    "# ocr = PaddleOCR(use_doc_orientation_classify=False, use_doc_unwarping=False) # text detection + textline orientation classification + text recognition\n",
    "# ocr = PaddleOCR(\n",
    "#     text_detection_model_name=\"PP-OCRv5_mobile_det\",\n",
    "#     text_recognition_model_name=\"PP-OCRv5_mobile_rec\",\n",
    "#     use_doc_orientation_classify=False,\n",
    "#     use_doc_unwarping=False,\n",
    "#     use_textline_orientation=False) # Switch to PP-OCRv5_mobile models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "e8d4cc65-f5e5-4922-8479-307048afdbff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Imagenes Cargadas\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"./imagenes\"  # ruta a tu carpeta con imágenes\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\"]\n",
    "\n",
    "# Filtramos solo imágenes\n",
    "image_files = sorted(\n",
    "    [os.path.join(image_dir, f) \n",
    "     for f in os.listdir(image_dir) \n",
    "     if os.path.splitext(f)[1].lower() in image_extensions],\n",
    "    key=lambda x: int(os.path.splitext(os.path.basename(x))[0])\n",
    ")\n",
    "\n",
    "print(str(len(image_files))+\" Imagenes Cargadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "c4e50d03-aaaa-4189-aaa2-286aed08bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_simple(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "     # lista por defecto a eliminar (puedes ampliarla)\n",
    "    default_remove = [\"continua...\", \"continua\", \"ejemplo\", \"borrar\", \"...\",\n",
    "    \"DEDUCIR\",\n",
    "    \"EL\",\n",
    "    \"COSTO\",\n",
    "    \"DE\",\n",
    "    \"REACONDICIONAMIENTO\",\n",
    "    \"Linea\",\n",
    "    \"Nueva\",\n",
    "    \"Unidades\", \"Usadas\",\"-\",\"Actualizacion\",\n",
    "    \"Unidades\",\n",
    "    \"Nuevas\",\n",
    "    \".\",\n",
    "    \"Precios\",\n",
    "    \":\",\n",
    "    \"Lista\",\n",
    "    \"-\",\n",
    "    \"N D.\"\n",
    "    ]\n",
    "    removes = default_remove\n",
    "\n",
    "    s = text.strip()\n",
    "\n",
    "    # normalizar y eliminar diacríticos (á -> a, ñ -> n, etc.)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = s.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "    # eliminar apostrofes rectos y curvos y guion bajo\n",
    "    s = re.sub(r\"[\\'\\u2019_]\", \"\", s)\n",
    "\n",
    "    # construir patrón para eliminar palabras/frases como palabras completas\n",
    "    # normalizamos las frases a ascii (ya lo hicimos arriba) y escapamos\n",
    "    # usamos boundaries \\b para evitar quitar partes dentro de otras palabras\n",
    "    escaped = [re.escape(r) for r in removes if r]\n",
    "    if escaped:\n",
    "        pattern = r\"\\b(?:\" + \"|\".join(escaped) + r\")\\b\"\n",
    "        s = re.sub(pattern, \" \", s, flags=re.IGNORECASE)\n",
    "\n",
    "    # remover múltiples espacios y strip final\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915f874-1dec-45b7-a670-cf6e909495cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ocr_to_multidimensional_sections(result, line_gap=6.5, cortes=None):\n",
    "    texts = result[\"rec_texts\"]\n",
    "    boxes = result[\"rec_boxes\"]\n",
    "\n",
    "    # Extraer coordenadas y asociar texto\n",
    "    data = []\n",
    "    for text, box in zip(texts, boxes):\n",
    "        x_min, y_min = box[0], box[1]  # esquina superior izq\n",
    "        text = clean_text_simple(text)\n",
    "        data.append({\"text\": text, \"x\": x_min, \"y\": y_min})\n",
    "\n",
    "    # Ordenar primero por Y (vertical) y luego por X (horizontal)\n",
    "    data = sorted(data, key=lambda d: (d[\"y\"], d[\"x\"]))\n",
    "\n",
    "    # Agrupamos por líneas (Y)\n",
    "    ordered_lines = []\n",
    "    current_line = []\n",
    "    last_y = None\n",
    "\n",
    "    for item in data:\n",
    "        if last_y is None or abs(item[\"y\"] - last_y) <= line_gap:\n",
    "            current_line.append(item)\n",
    "        else:\n",
    "            ordered_lines.append(current_line)\n",
    "            current_line = [item]\n",
    "        last_y = item[\"y\"]\n",
    "\n",
    "    if current_line:\n",
    "        ordered_lines.append(current_line)\n",
    "\n",
    "    # Si no se pasan cortes, devolvemos normal\n",
    "    if not cortes:\n",
    "        max_len = max(len(line) for line in ordered_lines)\n",
    "        ordered_filled = [\n",
    "            [i[\"text\"] for i in line] + [\"\"] * (max_len - len(line))\n",
    "            for line in ordered_lines\n",
    "        ]\n",
    "        return pd.DataFrame(ordered_filled)\n",
    "\n",
    "    # --- Usar cortes en X para dividir en secciones ---\n",
    "    cortes = sorted(cortes)  # [178, 286, 373] por ejemplo\n",
    "    n_sections = len(cortes) + 1\n",
    "\n",
    "    all_rows = []\n",
    "    for line in ordered_lines:\n",
    "        row = [\"\"] * n_sections\n",
    "        for item in line:\n",
    "            x = item[\"x\"]\n",
    "            text = item[\"text\"]\n",
    "\n",
    "            # Buscar en qué sección cae\n",
    "            section_idx = 0\n",
    "            for c in cortes:\n",
    "                if x > c:\n",
    "                    section_idx += 1\n",
    "                else:\n",
    "                    break\n",
    "            row[section_idx] += (\" \" + text if row[section_idx] else text)\n",
    "\n",
    "        all_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "fbd93a33-81cf-4aa6-8f9d-260ab8d0b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_imagenes_con_lineas(promedios):\n",
    "    output_dir = \"./imagenes_con_lineas\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"❌ No se pudo leer: {img_path}\")\n",
    "            continue\n",
    "    \n",
    "        # Obtener dimensiones\n",
    "        height, width, _ = img.shape\n",
    "    \n",
    "        # Dibujar cada línea\n",
    "        for x in promedios:\n",
    "            color = (0, 255, 0)  # Verde\n",
    "            thickness = 2\n",
    "            cv2.line(img, (int(x), 0), (int(x), height), color, thickness)\n",
    "    \n",
    "        # Guardar nueva imagen\n",
    "        output_path = os.path.join(output_dir, os.path.basename(img_path))\n",
    "        cv2.imwrite(output_path, img)\n",
    "        print(f\"✅ Guardada: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "2b860374-a5d4-4fda-9492-e3942799fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuario dijo NO\n",
      "./imagenes/20.jpg\n",
      "./imagenes/21.jpg\n",
      "./imagenes/22.jpg\n",
      "./imagenes/23.jpg\n",
      "./imagenes/24.jpg\n",
      "./imagenes/25.jpg\n",
      "./imagenes/26.jpg\n",
      "./imagenes/27.jpg\n",
      "./imagenes/28.jpg\n",
      "./imagenes/29.jpg\n"
     ]
    }
   ],
   "source": [
    "all_texts = {}\n",
    "\n",
    "promedios = [120,900,1250]\n",
    "\n",
    "resp = input(\"¿Deseas continuar? (s/n): \").strip().lower()\n",
    "if resp in (\"s\", \"si\", \"y\", \"yes\"):\n",
    "    print(\"Usuario dijo SÍ\")\n",
    "    crear_imagenes_con_lineas(promedios)\n",
    "else:\n",
    "    print(\"Usuario dijo NO\")\n",
    "\n",
    "for img_path in image_files[:10]:\n",
    "    print(img_path)\n",
    "    res = ocr.predict(img_path)\n",
    "\n",
    "    df_to_add = ocr_to_multidimensional_sections(res[0], line_gap=6.5, cortes=promedios)\n",
    "    #print(df_to_add)\n",
    "    all_texts[os.path.basename(img_path)] = df_to_add\n",
    "#print(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3837ed2-356e-49fc-b16c-51202241a5e0",
   "metadata": {},
   "source": [
    "# Guardar todo en un solo Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9ded0eb9-73d6-44b4-8ff2-b32bad3fd940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuario dijo NO\n"
     ]
    }
   ],
   "source": [
    "excel_path = \"ocr_resultado_completo.xlsx\"\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "final_df = pd.concat(all_texts.values(), ignore_index=True)\n",
    "\n",
    "# Eliminar filas donde TODAS las columnas estén vacías o con espacios\n",
    "final_df = final_df.replace(r'^\\s*$', np.nan, regex=True)  # convierte espacios en NaN\n",
    "final_df = final_df.dropna(how='all')  # elimina filas donde todo es NaN\n",
    "\n",
    "resp_save_original = input(\"¿Deseas continuar? (s/n): \").strip().lower()\n",
    "if resp_save_original in (\"s\", \"si\", \"y\", \"yes\"):\n",
    "    print(\"Usuario dijo SÍ\")\n",
    "    final_df.to_excel(\"ocr_origen.xlsx\", index=False, header=False)\n",
    "else:\n",
    "    print(\"Usuario dijo NO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8ffce423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_data_in_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c6798d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos_referencia(json_path):\n",
    "    \"\"\"Carga y procesa el JSON de marcas y modelos\"\"\"\n",
    "    json_data = leer_data_in_json(json_path)[\"rows\"]\n",
    "    \n",
    "    marcas_validas = set()\n",
    "    modelos_por_marca = {}\n",
    "    \n",
    "    for row in json_data:\n",
    "        marca = row[\"marca\"].upper()\n",
    "        modelo = row[\"modelo\"].upper()\n",
    "        \n",
    "        marcas_validas.add(marca)\n",
    "        if marca not in modelos_por_marca:\n",
    "            modelos_por_marca[marca] = set()\n",
    "        modelos_por_marca[marca].add(modelo)\n",
    "    \n",
    "    return marcas_validas, modelos_por_marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6afe47c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_en_fila(row, columnas_indices, valores_buscar):\n",
    "    \"\"\"\n",
    "    Busca valores en múltiples columnas de una fila\n",
    "    \n",
    "    Args:\n",
    "        row: fila del DataFrame\n",
    "        columnas_indices: lista de índices de columnas a buscar [0,1,2,3]\n",
    "        valores_buscar: set de valores a buscar\n",
    "    \n",
    "    Returns:\n",
    "        valor encontrado o None\n",
    "    \"\"\"\n",
    "    for idx in columnas_indices:\n",
    "        if idx < len(row):\n",
    "            valor_raw = row.iloc[idx]\n",
    "            if pd.notna(valor_raw):\n",
    "                valor = str(valor_raw).strip().upper()\n",
    "                if valor and valor in valores_buscar:\n",
    "                    return valor\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "71ec98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_marcas_modelos(df, marcas_validas, modelos_por_marca, columnas=[0, 1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Detecta y propaga marcas y modelos en el DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a procesar\n",
    "        marcas_validas: set de marcas válidas\n",
    "        modelos_por_marca: dict {marca: set(modelos)}\n",
    "        columnas: lista de índices de columnas donde buscar\n",
    "    \n",
    "    Returns:\n",
    "        tuple (lista_marcas, lista_modelos)\n",
    "    \"\"\"\n",
    "    marcas = []\n",
    "    modelos = []\n",
    "    marca_actual = None\n",
    "    modelo_actual = None\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Buscar marca en cualquier columna\n",
    "        marca_encontrada = buscar_en_fila(row, columnas, marcas_validas)\n",
    "        if marca_encontrada:\n",
    "            marca_actual = marca_encontrada\n",
    "            # ✅ Solo resetea si la marca REALMENTE cambió\n",
    "            if marca_actual != (marcas[-1] if marcas else None):\n",
    "                modelo_actual = None\n",
    "            print(f\"Fila {idx}: Nueva marca → {marca_actual}\")\n",
    "\n",
    "        # Buscar modelo\n",
    "        if marca_actual and marca_actual in modelos_por_marca:\n",
    "            modelo_encontrado = buscar_en_fila(row, columnas, modelos_por_marca[marca_actual])\n",
    "            if modelo_encontrado:\n",
    "                modelo_actual = modelo_encontrado\n",
    "                print(f\"Fila {idx}: Nuevo modelo → {modelo_actual}\")\n",
    "            # ✅ Si no encuentra modelo, mantiene modelo_actual (propagación)\n",
    "        \n",
    "        marcas.append(marca_actual)\n",
    "        modelos.append(modelo_actual)\n",
    "    \n",
    "    return marcas, modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a8fd1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separar_anio_y_resto_mejorado(texto):\n",
    "    \"\"\"Separa año del resto del texto, detectando patrones como 2024, 2023Q2, etc.\"\"\"\n",
    "    if pd.isna(texto) or texto == \"\":\n",
    "        return pd.Series([np.nan, \"\"])\n",
    "    \n",
    "    texto_str = str(texto).strip()\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Patrón 1: Año seguido de Q y número (2023Q2, 2024Q1, etc.)\n",
    "    match = re.match(r'^(\\d{4})Q\\d+', texto_str, re.IGNORECASE)\n",
    "    if match:\n",
    "        anio = match.group(1)\n",
    "        print(f\"  Detectado año con trimestre: {texto_str} → año: {anio}\")\n",
    "        return pd.Series([anio, \"\"])\n",
    "    \n",
    "    # Patrón 2: Año seguido de espacio y texto (2024 5p Dynamic...)\n",
    "    match = re.match(r'^(\\d{4})\\s+(.+)', texto_str)\n",
    "    if match:\n",
    "        return pd.Series([match.group(1), match.group(2)])\n",
    "    \n",
    "    # Patrón 3: Solo año (2024, 2023, etc.)\n",
    "    match = re.match(r'^(\\d{4})$', texto_str)\n",
    "    if match:\n",
    "        return pd.Series([match.group(1), \"\"])\n",
    "    \n",
    "    return pd.Series([np.nan, texto_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "48bf5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_excel(input_df, json_path, output_path):\n",
    "    \"\"\"Función principal que procesa el Excel completo\"\"\"\n",
    "    \n",
    "    # 1. Cargar datos de referencia\n",
    "    print(\"=== Cargando datos de referencia ===\")\n",
    "    marcas_validas, modelos_por_marca = cargar_datos_referencia(json_path)\n",
    "    print(f\"Marcas encontradas: {len(marcas_validas)}\")\n",
    "    print(f\"Total de modelos: {sum(len(v) for v in modelos_por_marca.values())}\\n\")\n",
    "    \n",
    "    # 2. Copiar DataFrame\n",
    "    df = input_df.copy()\n",
    "    \n",
    "    # 3. Detectar marcas y modelos en columnas A, B, C, D\n",
    "    print(\"=== Detectando marcas y modelos ===\")\n",
    "    marcas, modelos = detectar_marcas_modelos(\n",
    "        df, \n",
    "        marcas_validas, \n",
    "        modelos_por_marca,\n",
    "        columnas=[0, 1, 2, 3]  # A, B, C, D\n",
    "    )\n",
    "    \n",
    "    df[\"marca\"] = marcas\n",
    "    df[\"modelo\"] = modelos\n",
    "    \n",
    "    # 4. Detectar año y texto restante\n",
    "    print(\"\\n=== Procesando años ===\")\n",
    "    anio_resto = df.iloc[:, 1].apply(separar_anio_y_resto_mejorado)\n",
    "    df[\"año\"] = anio_resto[0]\n",
    "    df[\"texto\"] = anio_resto[1]\n",
    "    \n",
    "    # 5. Propagar año hacia abajo\n",
    "    df[\"año\"] = df[\"año\"].ffill()\n",
    "    \n",
    "    # 6. Detectar versiones\n",
    "    print(\"\\n=== Detectando versiones ===\")\n",
    "    versiones = []\n",
    "    for idx, texto in enumerate(df[\"texto\"]):\n",
    "        modelo_actual = df[\"modelo\"].iloc[idx]\n",
    "        \n",
    "        if modelo_actual and isinstance(texto, str) and texto.strip():\n",
    "            texto_upper = texto.strip().upper()\n",
    "            # Si el texto NO es el modelo, es una versión\n",
    "            if texto_upper != modelo_actual:\n",
    "                versiones.append(texto.strip())\n",
    "            else:\n",
    "                versiones.append(np.nan)\n",
    "        else:\n",
    "            versiones.append(np.nan)\n",
    "    \n",
    "    df[\"version\"] = versiones\n",
    "    \n",
    "     # 7. Añadir columnas de valores originales (columnas 2 y 3 = C y D)\n",
    "    df[\"valor_c\"] = df.iloc[:, 2] if len(df.columns) > 2 else np.nan\n",
    "    df[\"valor_d\"] = df.iloc[:, 3] if len(df.columns) > 3 else np.nan\n",
    "    \n",
    "    # 8. Reordenar columnas\n",
    "    columnas_base = [\"marca\", \"modelo\", \"año\", \"version\", \"valor_c\", \"valor_d\"]\n",
    "    columnas_extras = [col for col in df.columns \n",
    "                       if col not in columnas_base + [0, 1, 2, 3, \"texto\"]]\n",
    "    \n",
    "    excel_df = df[columnas_base + columnas_extras]\n",
    "    \n",
    "    # 8. Guardar\n",
    "    excel_df.to_excel(output_path, index=False, header=False)\n",
    "    \n",
    "    print(f\"\\n¡Listo! Archivo guardado en: {output_path}\")\n",
    "    print(f\"\\nEstadísticas:\")\n",
    "    print(f\"- Total filas procesadas: {len(excel_df)}\")\n",
    "    print(f\"- Marcas únicas detectadas: {excel_df['marca'].nunique()}\")\n",
    "    print(f\"- Modelos únicos detectados: {excel_df['modelo'].nunique()}\")\n",
    "    \n",
    "    return excel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "cf29f124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cargando datos de referencia ===\n",
      "Marcas encontradas: 111\n",
      "Total de modelos: 1525\n",
      "\n",
      "=== Detectando marcas y modelos ===\n",
      "Fila 0: Nueva marca → AUDI\n",
      "Fila 1: Nuevo modelo → A5\n",
      "Fila 85: Nueva marca → AUDI\n",
      "Fila 209: Nueva marca → AUDI\n",
      "Fila 228: Nuevo modelo → A6\n",
      "Fila 315: Nueva marca → AUDI\n",
      "Fila 349: Nuevo modelo → A7\n",
      "Fila 406: Nueva marca → AUDI\n",
      "Fila 445: Nuevo modelo → A8\n",
      "Fila 490: Nueva marca → AUDI\n",
      "Fila 609: Nueva marca → AUDI\n",
      "Fila 725: Nueva marca → AUDI\n",
      "Fila 798: Nueva marca → AUDI\n",
      "Fila 932: Nueva marca → AUDI\n",
      "\n",
      "=== Procesando años ===\n",
      "  Detectado año con trimestre: 2023Q2 → año: 2023\n",
      "  Detectado año con trimestre: 2021Q2 → año: 2021\n",
      "  Detectado año con trimestre: 2016Q5 → año: 2016\n",
      "  Detectado año con trimestre: 2015Q5 → año: 2015\n",
      "  Detectado año con trimestre: 2023Q7 → año: 2023\n",
      "  Detectado año con trimestre: 2022Q7 → año: 2022\n",
      "\n",
      "=== Detectando versiones ===\n",
      "\n",
      "¡Listo! Archivo guardado en: ocr_resultado_completo.xlsx\n",
      "\n",
      "Estadísticas:\n",
      "- Total filas procesadas: 995\n",
      "- Marcas únicas detectadas: 1\n",
      "- Modelos únicos detectados: 4\n",
      "\n",
      "=== Primeras 20 filas del resultado ===\n",
      "   marca modelo   año                                     version    valor_c  \\\n",
      "0   AUDI   None   NaN                                         NaN          A   \n",
      "1   AUDI     A5   NaN                                         NaN        NaN   \n",
      "2   AUDI     A5  2025                                      A5 . :        NaN   \n",
      "4   AUDI     A5  2025              2p 40 Select L4/2 0/T Aut MHEV  1,004,900   \n",
      "5   AUDI     A5  2025          2p 40 S Line L4/2 0/190/T Aut MHEV  1,094,900   \n",
      "6   AUDI     A5  2025  2p 45 S Line L4/2 0/249/T Aut Quattro MHEV        NaN   \n",
      "7   AUDI     A5  2025                                         NaN  1,224,900   \n",
      "8   AUDI     A5  2025                          2p S5 V6/3 0/T Aut        NaN   \n",
      "9   AUDI     A5  2025                                         NaN    0066000   \n",
      "10  AUDI     A5  2025                         2p RS5 V6/2 9/T Aut        NaN   \n",
      "11  AUDI     A5  2025                                         NaN    0066T6T   \n",
      "12  AUDI     A5  2025                                   Sportback        NaN   \n",
      "13  AUDI     A5  2025              4p 40 Select L4/2 0/T Aut MHEV  1,004,900   \n",
      "14  AUDI     A5  2025          4p 40 S Line L4/2 0/190/T Aut MHEV  1,094,900   \n",
      "15  AUDI     A5  2025  4p 45 S Line L4/2 0/252/T Aut Quattro MHEV        NaN   \n",
      "16  AUDI     A5  2025                                         NaN  1,224,900   \n",
      "17  AUDI     A5  2025                          4p S5 V6/3 0/T Aut   00660000   \n",
      "18  AUDI     A5  2025                         4p RS5 V6/2 9/T Aut  1,919,900   \n",
      "19  AUDI     A5  2025                                       Sedan        NaN   \n",
      "20  AUDI     A5  2025          4p Sedan Dynamic L4/2 0/T Aut MHEV    974,900   \n",
      "\n",
      "   valor_d  \n",
      "0        C  \n",
      "1      NaN  \n",
      "2      NaN  \n",
      "4      NaN  \n",
      "5      NaN  \n",
      "6      NaN  \n",
      "7      NaN  \n",
      "8      NaN  \n",
      "9      NaN  \n",
      "10     NaN  \n",
      "11     NaN  \n",
      "12     NaN  \n",
      "13     NaN  \n",
      "14     NaN  \n",
      "15     NaN  \n",
      "16     NaN  \n",
      "17     NaN  \n",
      "18     NaN  \n",
      "19     NaN  \n",
      "20     NaN  \n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# EJECUCIÓN PRINCIPAL\n",
    "# =====================\n",
    "\n",
    "# Usar la función original separar_anio_y_resto si ya la tienes definida\n",
    "# Si no, se usa la versión mejorada de arriba\n",
    "\n",
    "resultado = procesar_excel(\n",
    "    input_df=final_df,\n",
    "    json_path=\"data.json\",\n",
    "    output_path=excel_path\n",
    ")\n",
    "\n",
    "# Mostrar primeras filas\n",
    "print(\"\\n=== Primeras 20 filas del resultado ===\")\n",
    "print(resultado.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
