{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff38bbbc-c227-475c-9181-97ff56f5d977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mChecking connectivity to the model hosters, this may take a while. To bypass this check, set `DISABLE_MODEL_SOURCE_CHECK` to `True`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerias Cargadas\n"
     ]
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import json\n",
    "import cv2\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Opcional: más columnas visibles en la salida\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "\n",
    "print(\"Librerias Cargadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9ccefe-aec5-45a5-911f-1745cb335035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/PP-LCNet_x1_0_textline_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('latin_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/root/.paddlex/official_models/latin_PP-OCRv5_mobile_rec`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#ocr = PaddleOCR(\n",
    "#    use_doc_orientation_classify=False, \n",
    " #   use_doc_unwarping=False, \n",
    " #   use_textline_orientation=False) # text detection + text recognition\n",
    "\n",
    "# text image preprocessing + text detection + textline orientation classification + text recognition\n",
    "ocr = PaddleOCR(use_doc_orientation_classify=False, use_doc_unwarping=False, lang=\"es\")\n",
    "\n",
    "# ocr = PaddleOCR(use_doc_orientation_classify=False, use_doc_unwarping=False) # text detection + textline orientation classification + text recognition\n",
    "# ocr = PaddleOCR(\n",
    "#     text_detection_model_name=\"PP-OCRv5_mobile_det\",\n",
    "#     text_recognition_model_name=\"PP-OCRv5_mobile_rec\",\n",
    "#     use_doc_orientation_classify=False,\n",
    "#     use_doc_unwarping=False,\n",
    "#     use_textline_orientation=False) # Switch to PP-OCRv5_mobile models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d4cc65-f5e5-4922-8479-307048afdbff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 Imagenes Cargadas\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"./imagenes\"  # ruta a tu carpeta con imágenes\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\"]\n",
    "\n",
    "# Filtramos solo imágenes\n",
    "image_files = sorted(\n",
    "    [os.path.join(image_dir, f) \n",
    "     for f in os.listdir(image_dir) \n",
    "     if os.path.splitext(f)[1].lower() in image_extensions],\n",
    "    key=lambda x: int(os.path.splitext(os.path.basename(x.split(\"_\")[1]))[0])\n",
    ")\n",
    "\n",
    "print(str(len(image_files))+\" Imagenes Cargadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d915f874-1dec-45b7-a670-cf6e909495cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ocr_to_multidimensional_sections(result, line_gap=6.5, cortes=None):\n",
    "    texts = result[\"rec_texts\"]\n",
    "    boxes = result[\"rec_boxes\"]\n",
    "\n",
    "    # Extraer coordenadas y asociar texto\n",
    "    data = []\n",
    "    for text, box in zip(texts, boxes):\n",
    "        x_min, y_min = box[0], box[1]  # esquina superior izq\n",
    "        text = clean_text_regex(text)\n",
    "        data.append({\"text\": text, \"x\": x_min, \"y\": y_min})\n",
    "\n",
    "    # Ordenar primero por Y (vertical) y luego por X (horizontal)\n",
    "    data = sorted(data, key=lambda d: (d[\"y\"], d[\"x\"]))\n",
    "\n",
    "    # Agrupamos por líneas (Y)\n",
    "    ordered_lines = []\n",
    "    current_line = []\n",
    "    last_y = None\n",
    "\n",
    "    for item in data:\n",
    "        if last_y is None or abs(item[\"y\"] - last_y) <= line_gap:\n",
    "            current_line.append(item)\n",
    "        else:\n",
    "            ordered_lines.append(current_line)\n",
    "            current_line = [item]\n",
    "        last_y = item[\"y\"]\n",
    "\n",
    "    if current_line:\n",
    "        ordered_lines.append(current_line)\n",
    "\n",
    "    # Si no se pasan cortes, devolvemos normal\n",
    "    if not cortes:\n",
    "        max_len = max(len(line) for line in ordered_lines)\n",
    "        ordered_filled = [\n",
    "            [i[\"text\"] for i in line] + [\"\"] * (max_len - len(line))\n",
    "            for line in ordered_lines\n",
    "        ]\n",
    "        return pd.DataFrame(ordered_filled)\n",
    "\n",
    "    # --- Usar cortes en X para dividir en secciones ---\n",
    "    cortes = sorted(cortes)  # [178, 286, 373] por ejemplo\n",
    "    n_sections = len(cortes) + 1\n",
    "\n",
    "    all_rows = []\n",
    "    for line in ordered_lines:\n",
    "        row = [\"\"] * n_sections\n",
    "        for item in line:\n",
    "            x = item[\"x\"]\n",
    "            text = item[\"text\"]\n",
    "\n",
    "            # Buscar en qué sección cae\n",
    "            section_idx = 0\n",
    "            for c in cortes:\n",
    "                if x > c:\n",
    "                    section_idx += 1\n",
    "                else:\n",
    "                    break\n",
    "            row[section_idx] += (\" \" + text if row[section_idx] else text)\n",
    "\n",
    "        all_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e50d03-aaaa-4189-aaa2-286aed08bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilamos el patrón una sola vez (mejor rendimiento si llamas mucho a la función)\n",
    "def _build_remove_regex(removes):\n",
    "    \"\"\"\n",
    "    Genera un patrón que:\n",
    "      - Ignora acentos (la función principal normaliza a ASCII).\n",
    "      - Acepta separadores entre palabras (espacio, guion, underscore).\n",
    "      - Permite variaciones de puntuación al final (... !!! ???).\n",
    "      - Quita como token completo (no dentro de otras palabras).\n",
    "      - Acepta pluralización simple: s | es (opcional).\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for raw in removes:\n",
    "        if not raw:\n",
    "            continue\n",
    "        # normalizar a ASCII (coincidir sin acentos)\n",
    "        base = unicodedata.normalize(\"NFKD\", raw).encode(\"ascii\", \"ignore\").decode(\"ascii\").lower().strip()\n",
    "        if not base:\n",
    "            continue\n",
    "\n",
    "        # escapar y permitir separadores entre palabras (si hay espacios)\n",
    "        # \"guia rapida\" -> \"guia[\\\\s_\\\\-]+rapida\"\n",
    "        token = re.escape(base).replace(r\"\\ \", r\"[\\s_\\-]+\")\n",
    "\n",
    "        # permitir puntos/símbolos finales repetidos\n",
    "        # e.g., \"continua...\" -> \"continua[\\.\\!\\?\\u2026]*\"\n",
    "        token = fr\"{token}(?:[\\-\\.\\!\\?\\u2026]+)?\"\n",
    "\n",
    "        # plural simple español (s|es) opcional cuando termina en letra\n",
    "        # (solo añadimos si la última es alfanumérica)\n",
    "        if re.search(r\"[a-z0-9]$\", base):\n",
    "            token = fr\"{token}(?:es|s)?\"\n",
    "\n",
    "        # asegurar “borde” de palabra a la española tras normalizar (ASCII):\n",
    "        # que no esté pegado a letras/dígitos por los lados\n",
    "        token = fr\"(?<![A-Za-z0-9])(?:{token})(?![A-Za-z0-9])\"\n",
    "        parts.append(token)\n",
    "\n",
    "    if not parts:\n",
    "        # algo que nunca coincida\n",
    "        return re.compile(r\"(?!x)x\", flags=re.IGNORECASE)\n",
    "\n",
    "    big = \"|\".join(parts)\n",
    "    return re.compile(big, flags=re.IGNORECASE)\n",
    "\n",
    "# lista por defecto (puedes ampliarla)\n",
    "_DEFAULT_REMOVES = [\n",
    "    \"continua...\", \"continua\", \"ejemplo\", \"borrar\", \"unidades\", \"nuevas\" ,\"suv\",\n",
    "    \"linea\",\"nueva\",\"Usadas\",\"Actualizacion\", \"UnidadesNuevas\", \"Precios\", \"de:\"\n",
    "    # agrega aquí más frases/palabras\n",
    "]\n",
    "_REMOVE_RE = _build_remove_regex(_DEFAULT_REMOVES)\n",
    "\n",
    "def clean_text_regex(text, extra_removes=None):\n",
    "    \"\"\"\n",
    "    Limpia 'text' eliminando tokens molestos con regex robusto.\n",
    "    - extra_removes: lista adicional de términos/frases a quitar.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # normaliza y elimina diacríticos (á->a, ñ->n)\n",
    "    s = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "    # quita comillas curvas/rectas y underscores sueltos repetidos\n",
    "    s = re.sub(r\"[\\'\\u2019_]+\", \"\", s)\n",
    "\n",
    "    # compila patrón combinado si hay extras\n",
    "    if extra_removes:\n",
    "        combined = list(_DEFAULT_REMOVES) + list(extra_removes)\n",
    "        remove_re = _build_remove_regex(combined)\n",
    "    else:\n",
    "        remove_re = _REMOVE_RE\n",
    "\n",
    "    # elimina tokens objetivo\n",
    "    s = remove_re.sub(\" \", s)\n",
    "\n",
    "    # colapsa espacios y limpia\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f57255c-84f9-4df7-9756-be980ff54a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "limite_indice = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbd93a33-81cf-4aa6-8f9d-260ab8d0b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_imagenes_con_lineas(promedios, limite):\n",
    "    output_dir = \"./imagenes_con_lineas\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for img_path in image_files[limite_indice:limite]:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\" No se pudo leer: {img_path}\")\n",
    "            continue\n",
    "    \n",
    "        # Obtener dimensiones\n",
    "        height, width, _ = img.shape\n",
    "    \n",
    "        # Dibujar cada línea\n",
    "        for x in promedios:\n",
    "            color = (0, 255, 0)  # Verde\n",
    "            thickness = 2\n",
    "            cv2.line(img, (int(x), 0), (int(x), height), color, thickness)\n",
    "    \n",
    "        # Guardar nueva imagen\n",
    "        output_path = os.path.join(output_dir, os.path.basename(img_path))\n",
    "        cv2.imwrite(output_path, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b860374-a5d4-4fda-9492-e3942799fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "¿Deseas continuar? (s/n):  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuario dijo SÍ\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "¿Cuántas páginas deseas procesar? (0 = todas):  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: ./imagenes/img_010.jpg\n"
     ]
    }
   ],
   "source": [
    "all_texts = {}\n",
    "promedios = [100,800,970]\n",
    "\n",
    "resp = input(\"¿Deseas continuar? (s/n): \").strip().lower()\n",
    "\n",
    "if resp in (\"s\", \"si\", \"y\", \"yes\"):\n",
    "    print(\"Usuario dijo SÍ\")\n",
    "\n",
    "    # Segunda pregunta\n",
    "    raw = input(\"¿Cuántas páginas deseas procesar? (0 = todas): \").strip()\n",
    "    try:\n",
    "        num = int(raw)\n",
    "    except ValueError:\n",
    "        print(\"Número inválido:\", raw)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if num < 0:\n",
    "        print(\"El número no puede ser negativo.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # si marca 0, toma todas las páginas\n",
    "    if num == 0:\n",
    "        num = len(image_files)\n",
    "        print(f\"Procesando todas las {num} páginas detectadas...\")\n",
    "\n",
    "    # limitar por si num > len(image_files)\n",
    "    num = min(num, len(image_files))\n",
    "\n",
    "    crear_imagenes_con_lineas(promedios, num)\n",
    "\n",
    "    # recorrer solo las páginas solicitadas\n",
    "    for img_path in image_files[limite_indice:num]:\n",
    "        print(f\"Procesando: {img_path}\")\n",
    "        res = ocr.predict(img_path)\n",
    "        df_to_add = ocr_to_multidimensional_sections(\n",
    "            res[0], line_gap=6.5, cortes=promedios\n",
    "        )\n",
    "        all_texts[os.path.basename(img_path)] = df_to_add\n",
    "\n",
    "else:\n",
    "    print(\"Usuario dijo NO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3837ed2-356e-49fc-b16c-51202241a5e0",
   "metadata": {},
   "source": [
    "# Guardar todo en un solo Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9073e11c-e8bf-4197-bffe-eebbd7da9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s2 = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s2 = s2.lower().strip()\n",
    "    s2 = re.sub(r\"[^\\w\\s-]+\", \" \", s2)      # elimina signos\n",
    "    s2 = re.sub(r\"[_\\s\\-]+\", \" \", s2).strip()\n",
    "    return s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c86bd7f-8ab7-4669-bc36-dd8ff59c4d11",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'marcas.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#cargar marcas\u001b[39;00m\n\u001b[1;32m      2\u001b[0m marcas_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarcas.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# ajusta ruta si es necesario\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m marcas_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarcas_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarca\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m marcas_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEl CSV de marcas debe tener una columna llamada \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarca\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'marcas.csv'"
     ]
    }
   ],
   "source": [
    "#cargar marcas\n",
    "marcas_path = \"marcas.csv\"  # ajusta ruta si es necesario\n",
    "marcas_df = pd.read_csv(marcas_path)\n",
    "\n",
    "if \"marca\" not in marcas_df.columns:\n",
    "    raise ValueError(\"El CSV de marcas debe tener una columna llamada 'marca'.\")\n",
    "\n",
    "# mapa normalizado -> original (para conservar capitalización)\n",
    "marca_norm_to_original = {}\n",
    "for m in marcas_df[\"marca\"].dropna().astype(str):\n",
    "    key = normalize(m)\n",
    "    if key:\n",
    "        marca_norm_to_original[key] = m.strip()\n",
    "\n",
    "marca_norm_set = set(marca_norm_to_original.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded0eb9-73d6-44b4-8ff2-b32bad3fd940",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = \"ocr_resultado_completo.xlsx\"\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "final_df = pd.concat(all_texts.values(), ignore_index=True)\n",
    "\n",
    "# Eliminar filas donde TODAS las columnas estén vacías o con espacios\n",
    "final_df = final_df.replace(r'^\\s*$', np.nan, regex=True)  # convierte espacios en NaN\n",
    "final_df = final_df.dropna(how='all')  # elimina filas donde todo es NaN\n",
    "\n",
    "final_df.to_excel(\"ocr_origen.xlsx\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905bbfa-e5f3-4afc-9662-735c496dc847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separar_anio_y_resto(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return np.nan, texto, False\n",
    "    partes = texto.strip().split(maxsplit=1)\n",
    "    if len(partes) == 0:\n",
    "        return np.nan, texto, False\n",
    "    primer = partes[0]\n",
    "    resto = partes[1] if len(partes) > 1 else \"\"\n",
    "    if primer.isdigit() and 1900 <= int(primer) <= 2099:\n",
    "        return primer, resto , True\n",
    "    return np.nan, texto, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a89b8c-8f83-433e-bfd3-42ac3419d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_df(df, title=None, cols=None, n=50):\n",
    "    \"\"\"Muestra un título y un snapshot del DataFrame (o columnas seleccionadas).\"\"\"\n",
    "    if title:\n",
    "        display(HTML(f\"<h3 style='margin:8px 0'>{title}</h3>\"))\n",
    "    if cols is None:\n",
    "        display(df.head(n))\n",
    "    else:\n",
    "        # Muestra solo columnas existentes (evita KeyError si alguna falta)\n",
    "        cols = [c for c in cols if c in df.columns]\n",
    "        display(df[cols].head(n))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Copiamos y mostramos\n",
    "# ------------------------------------------------------------------\n",
    "df = final_df.copy()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Detectar año y resto desde la columna 1 (como en tu flujo)\n",
    "# ------------------------------------------------------------------\n",
    "anio_resto = df[1].apply(lambda x: pd.Series(separar_anio_y_resto(x)))\n",
    "\n",
    "if \"version\" not in df.columns:\n",
    "    df[\"version\"] = pd.Series(pd.NA, index=df.index)\n",
    "\n",
    "df[\"version\"] = df[\"version\"].ffill()\n",
    "\n",
    "df[\"año\"]   = anio_resto[0]\n",
    "df[\"texto\"] = anio_resto[1]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Propagar el año hacia abajo (ffill)\n",
    "# ------------------------------------------------------------------\n",
    "df[\"año\"] = df[\"año\"].ffill()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Detectar modelo y versión (tu lógica base)\n",
    "#    - texto corto (<=3 tokens) -> modelo\n",
    "#    - texto largo -> versión del modelo actual\n",
    "# ------------------------------------------------------------------\n",
    "modelos = []\n",
    "versiones = []\n",
    "modelo_actual = None\n",
    "\n",
    "for texto in df[\"texto\"]:\n",
    "    if isinstance(texto, str) and texto.strip() != \"\":\n",
    "        if len(texto.split()) <= 3:  # criterio: texto corto → modelo\n",
    "            modelo_actual = texto.strip()\n",
    "            modelos.append(modelo_actual)\n",
    "            versiones.append(np.nan)\n",
    "        else:\n",
    "            modelos.append(modelo_actual)\n",
    "            versiones.append(texto.strip())\n",
    "    else:\n",
    "        modelos.append(modelo_actual)\n",
    "        versiones.append(np.nan)\n",
    "\n",
    "df[\"modelo\"]  = modelos\n",
    "df[\"version\"] = versiones\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6) Agregar columna 'marca' (después de año) y 'id' (después de version)\n",
    "#    Aquí las creamos vacías; puedes llenarlas luego.\n",
    "# ------------------------------------------------------------------\n",
    "df = df.copy()  # tu DataFrame de trabajo que ya tiene la columna \"texto\"\n",
    "df[\"texto_norm\"] = df[\"texto\"].apply(normalize)\n",
    "\n",
    "# ¿la fila es exactamente una marca?\n",
    "df[\"marca_key\"] = df[\"texto_norm\"].where(df[\"texto_norm\"].isin(marca_norm_set))\n",
    "\n",
    "# asigna el nombre original de la marca donde corresponde\n",
    "df[\"marca_detectada\"] = df[\"marca_key\"].map(marca_norm_to_original)\n",
    "\n",
    "# propaga la marca hacia abajo: TODAS las filas tendrán marca (desde la primera detección)\n",
    "df[\"marca\"] = df[\"marca_detectada\"].ffill()\n",
    "\n",
    "df[\"id\"]    = np.nan\n",
    "#show_df(df, \"5) Agregadas columnas 'marca' e 'id'\", cols=[\"año\", \"marca\", \"modelo\", \"version\", \"id\"])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7) Reordenar columnas: año, marca, modelo, version, id, (resto)\n",
    "# ------------------------------------------------------------------\n",
    "columnas_ordenadas = (\n",
    "    [\"año\", \"marca\", \"modelo\", \"version\", \"id\"]\n",
    "    + [c for c in df.columns if c not in [0, 1, \"texto\", \"año\", \"marca\", \"modelo\", \"version\", \"id\"]]\n",
    ")\n",
    "excel_df = df[columnas_ordenadas]\n",
    "\n",
    "#display(excel_df)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8) Guardar a Excel (opcional)\n",
    "# ------------------------------------------------------------------\n",
    "excel_path = \"salida_modelos_versiones.xlsx\"  # ajusta ruta si lo deseas\n",
    "excel_df.to_excel(excel_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b87d33-8ef8-40e1-879e-9b5843437df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
